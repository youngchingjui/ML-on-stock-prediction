{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks on Stock Prediction v2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thesis: NN can better identify patterns within the stock market, because they are able to analyse data at dimensions higher than 3 very easily. People are only able to perceive up to 3 dimensions, which limits our ability to identify patterns within a multi-dimensional data set\n",
    "\n",
    "Practice: Build a RNN for a single stock. See if it can identify whether a stock will rise by 1% within 5 days."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps:\n",
    "- Check if other stock characteristics are important (industry, business model, volatility, etc.)\n",
    "- Expand to include multiple day windows (3 days, 10 days, 1 day, etc.) See if predictions are more accurate for these multiple windows\n",
    "- Expand to include multiple buffers (2%, 5%, 10%, etc.). \n",
    "    - Note: Of course a 10% rise in 5 days is very unlikely, but perhaps the model is able to predict these flags with higher precision?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import the required packages\n",
    "\n",
    "import numpy as np\n",
    "from numpy import array, argmax\n",
    "\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from pandas_datareader import data\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, LSTM\n",
    "from keras.utils import to_categorical, plot_model\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "\n",
    "import time\n",
    "\n",
    "from colour import Color\n",
    "\n",
    "import missingno as msno"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model:\n",
    "\n",
    "I'm going to build a Long-Short Term Memory Neural Network.\n",
    "\n",
    "The data:\n",
    "Each data sample will represent `p` days previous of trading information (OHLC) for a particular stock. The model will predict whether the stock will be able to go up `target`% within `window` number days.\n",
    "\n",
    "For now, we'll set the following hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set our hyperparameter variables\n",
    "\n",
    "# For now we'll just pick one stock. Later, I'll want to add a list of multiple stocks\n",
    "stockNames = [\"GE\", \"MA\", \"MSFT\", \"DIS\"] # Sample stock to play with\n",
    "target = .01     # Target to identify if stock increased by 1%\n",
    "window = 5       # Window to identify whether stock hit target within these number of days\n",
    "p = 90           # number of days of data for each data point\n",
    "\n",
    "# The features we'll include in X to inform the model\n",
    "features = [\"oh_pct\", \"ol_pct\", \"oc_pct\", \"co_pct\", \"cc_pct\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data for 4 stocks took 18.0 seconds\n"
     ]
    }
   ],
   "source": [
    "# Load the OHLC data into the variable data\n",
    "data_source = 'morningstar'\n",
    "start_date = datetime(2010, 1, 1)\n",
    "end_date = datetime(2018, 3, 2)\n",
    "\n",
    "# Measure how long it takes to load the data \n",
    "startTime = time.time()\n",
    "\n",
    "# Load the data\n",
    "base_data = data.DataReader(stockNames, data_source, start_date, end_date)\n",
    "column_labels = base_data.columns\n",
    "print(\"Loading data for {} stocks took {:3.1f} seconds\".format(len(stockNames), time.time()-startTime))\n",
    "\n",
    "# Sort the data by date so that we can reference slices of the date index later\n",
    "base_data.sort_index(level=[0], axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the base_data to CSV so we can check the data transformations\n",
    "\n",
    "base_data.to_csv(\"base_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a summary of our table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Close</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Open</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>8524.00000</td>\n",
       "      <td>8524.000000</td>\n",
       "      <td>8524.000000</td>\n",
       "      <td>8524.000000</td>\n",
       "      <td>8.524000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>52.41647</td>\n",
       "      <td>52.807143</td>\n",
       "      <td>51.992888</td>\n",
       "      <td>52.402731</td>\n",
       "      <td>2.611537e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>32.57289</td>\n",
       "      <td>32.770625</td>\n",
       "      <td>32.355952</td>\n",
       "      <td>32.566249</td>\n",
       "      <td>2.800132e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>13.88000</td>\n",
       "      <td>14.240000</td>\n",
       "      <td>13.750000</td>\n",
       "      <td>13.990000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>26.55000</td>\n",
       "      <td>26.743750</td>\n",
       "      <td>26.340000</td>\n",
       "      <td>26.557500</td>\n",
       "      <td>6.377135e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>39.91000</td>\n",
       "      <td>40.179500</td>\n",
       "      <td>39.565000</td>\n",
       "      <td>39.900000</td>\n",
       "      <td>1.660667e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>76.40000</td>\n",
       "      <td>76.930000</td>\n",
       "      <td>75.900000</td>\n",
       "      <td>76.482500</td>\n",
       "      <td>3.841560e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>177.87000</td>\n",
       "      <td>179.170000</td>\n",
       "      <td>176.060000</td>\n",
       "      <td>177.720000</td>\n",
       "      <td>4.313160e+08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Close         High          Low         Open        Volume\n",
       "count  8524.00000  8524.000000  8524.000000  8524.000000  8.524000e+03\n",
       "mean     52.41647    52.807143    51.992888    52.402731  2.611537e+07\n",
       "std      32.57289    32.770625    32.355952    32.566249  2.800132e+07\n",
       "min      13.88000    14.240000    13.750000    13.990000  0.000000e+00\n",
       "25%      26.55000    26.743750    26.340000    26.557500  6.377135e+06\n",
       "50%      39.91000    40.179500    39.565000    39.900000  1.660667e+07\n",
       "75%      76.40000    76.930000    75.900000    76.482500  3.841560e+07\n",
       "max     177.87000   179.170000   176.060000   177.720000  4.313160e+08"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's what the first few rows of what GE looks like\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Close</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Open</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Symbol</th>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">DIS</th>\n",
       "      <th>2010-01-01</th>\n",
       "      <td>32.25</td>\n",
       "      <td>32.75</td>\n",
       "      <td>32.22</td>\n",
       "      <td>32.27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-04</th>\n",
       "      <td>32.07</td>\n",
       "      <td>32.75</td>\n",
       "      <td>31.87</td>\n",
       "      <td>32.50</td>\n",
       "      <td>13700385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-05</th>\n",
       "      <td>31.99</td>\n",
       "      <td>32.16</td>\n",
       "      <td>31.70</td>\n",
       "      <td>32.07</td>\n",
       "      <td>10307697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-06</th>\n",
       "      <td>31.82</td>\n",
       "      <td>32.00</td>\n",
       "      <td>31.68</td>\n",
       "      <td>31.90</td>\n",
       "      <td>10709499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-07</th>\n",
       "      <td>31.83</td>\n",
       "      <td>31.86</td>\n",
       "      <td>31.54</td>\n",
       "      <td>31.77</td>\n",
       "      <td>8202059</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Close   High    Low   Open    Volume\n",
       "Symbol Date                                            \n",
       "DIS    2010-01-01  32.25  32.75  32.22  32.27         0\n",
       "       2010-01-04  32.07  32.75  31.87  32.50  13700385\n",
       "       2010-01-05  31.99  32.16  31.70  32.07  10307697\n",
       "       2010-01-06  31.82  32.00  31.68  31.90  10709499\n",
       "       2010-01-07  31.83  31.86  31.54  31.77   8202059"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Here's a summary of what our data looks like. \n",
    "print(\"Here's a summary of our table:\")\n",
    "display(base_data.describe())\n",
    "print(\"Here's what the first few rows of what {} looks like\".format(stockNames[0]))\n",
    "display(base_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create variables for creating flag column\n",
    "def get_trading_days(base_data):\n",
    "    stockNames = base_data.index.levels[0].values\n",
    "    trading_days = {}\n",
    "    for stock in stockNames:\n",
    "        trading_days[stock] = base_data.loc[stock].shape[0]\n",
    "    return trading_days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll want to create a flag for each day. The flag will indicate whether our stock price will hit the price `target` within `window` days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_is_sellable_flag(base_data, window = 5, target = 0.01):\n",
    "    # Identify the stocks within the base_data\n",
    "    stockNames = base_data.index.levels[0].values\n",
    "    \n",
    "    # Identify the number of trading days uploaded for each stock\n",
    "    trading_days = get_trading_days(base_data)\n",
    "    \n",
    "    # Check if a \"is_sellable\" column already exists. If so, delete it\n",
    "    if \"is_sellable\" in base_data.columns:\n",
    "        base_data.drop(labels=\"is_sellable\", axis=1, inplace=True)\n",
    "    \n",
    "    open_price = base_data.Open\n",
    "    high = base_data.High\n",
    "    \n",
    "    # Create a `is_sellable` flag column filled with 0s\n",
    "    is_sellable = pd.Series(0, index=base_data.index, name=\"is_sellable\")\n",
    "\n",
    "    startTime = time.time()\n",
    "    #TODO: Vectorize this part. It takes a long time to run.\n",
    "    # For loop to add the column with flags to identify if stock will hit target within window\n",
    "    for stock in stockNames:\n",
    "        index = 0\n",
    "        for indexDate in base_data.loc[stock].index:\n",
    "            endIndex = min(index + window, trading_days[stock]-1)\n",
    "            \n",
    "            # We take the endIndex - 1 because when slicing the dates \n",
    "            # for high_window below, the slice takes both the start and end times inclusive.\n",
    "            endDate = base_data.loc[stock].index[endIndex-1]\n",
    "            high_window = high.loc[stock,indexDate:endDate]\n",
    "\n",
    "            # If high price within the high_window is greater than target price, set is_sellable to 1\n",
    "            if (high_window > open_price.loc[stock,indexDate] * (1+target)).any():\n",
    "                is_sellable.loc[stock, indexDate] = 1\n",
    "            index += 1\n",
    "\n",
    "    # Add is_sellable column to the data variable\n",
    "    print(\"Creating is_sellable column took {:3.1f} seconds\".format(time.time()-startTime))\n",
    "    base_data = pd.concat((base_data, is_sellable), axis=1)\n",
    "    \n",
    "    return base_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating is_sellable column took 24.2 seconds\n"
     ]
    }
   ],
   "source": [
    "# Create the is_sellable flag column\n",
    "base_data = create_is_sellable_flag(base_data, window=window, target=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_prices(base_data):\n",
    "    \n",
    "    # Normalize data by creating columns showing price relation to Open price. Create 4 columns:\n",
    "    # 1) oh_pct = High / Open\n",
    "    # 2) ol_pct = Low / Open\n",
    "    # 3) oc_pct = Close / Open\n",
    "    # 4) co_pct = Open / Close(t-1)\n",
    "    # 5) cc_pct = Close / Close(t-1)\n",
    "    \n",
    "    # Track how long it takes to do this. Performance affected by for loop.\n",
    "    startTime = time.time()\n",
    "    \n",
    "    # Identify the stocks within the base_data\n",
    "    stockNames = base_data.index.levels[0].values\n",
    "    \n",
    "    # Identify the number of trading days uploaded for each stock\n",
    "    trading_days = get_trading_days(base_data)\n",
    "    \n",
    "    # Check if these columns already exist. If so, delete them\n",
    "    for col in base_data.columns:\n",
    "        if col in features:\n",
    "            base_data.drop(labels=col, axis=1, inplace=True)\n",
    "    \n",
    "    # Create the new normalized columns\n",
    "    oh_pct = base_data.High / base_data.Open - 1\n",
    "    ol_pct = base_data.Low / base_data.Open - 1\n",
    "    oc_pct = base_data.Close / base_data.Open - 1\n",
    "    \n",
    "    # Name the new columns, so they have a header in the table\n",
    "    oh_pct.name = \"oh_pct\"\n",
    "    ol_pct.name = \"ol_pct\"\n",
    "    oc_pct.name = \"oc_pct\"\n",
    "    \n",
    "    # Need to run these on a foor loop because they look back 1 day\n",
    "    co_pct = pd.Series(0., index=base_data.index, name=\"co_pct\")\n",
    "    cc_pct = pd.Series(0., index=base_data.index, name=\"cc_pct\")\n",
    "    \n",
    "    for stock in stockNames:\n",
    "        for i in np.arange(trading_days[stock]):\n",
    "            if i != 0:\n",
    "                co_pct.loc[stock].iloc[i] = base_data.Open.loc[stock].iloc[i] / base_data.Close.loc[stock].iloc[i-1] - 1\n",
    "                cc_pct.loc[stock].iloc[i] = base_data.Close.loc[stock].iloc[i] / base_data.Close.loc[stock].iloc[i-1] - 1\n",
    "    base_data = pd.concat((base_data, oh_pct, ol_pct, oc_pct, co_pct, cc_pct), axis=1)\n",
    "    print(\"Creating these columns for {} stocks took {:3.1f} seconds\".format(len(stockNames), time.time()-startTime))\n",
    "    return base_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create columns with % change in prices, instead of raw price numbers\n",
    "base_data = normalize_prices(base_data)\n",
    "base_data.to_csv(\"normalized.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stock_prices(base_data):\n",
    "    col_to_remove = [\"Close\", \"High\", \"Low\", \"Open\", \"Volume\"]\n",
    "    for col in col_to_remove:\n",
    "        if col in base_data.columns:\n",
    "            base_data.drop(col, axis=1, inplace=True)\n",
    "    return base_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll also have to remove the original stock prices\n",
    "base_data = remove_stock_prices(base_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_graphs(base_data, ncols = 3):\n",
    "\n",
    "    # Displays graphs showing the stock data\n",
    "    \n",
    "    # Identify the stocks within the base_data\n",
    "    stockNames = base_data.index.levels[0].values\n",
    "    \n",
    "    # Identify the number of trading days uploaded for each stock\n",
    "    trading_days = get_trading_days(base_data)\n",
    "    \n",
    "    # Identify which stock has the most trading days\n",
    "    longest_stock = max(trading_days)\n",
    "    max_time_index = base_data.loc[longest_stock].index\n",
    "    x = max_time_index\n",
    "    \n",
    "    # Setup a few variables for creating the graphs\n",
    "    nrows = int(np.ceil(len(stockNames)/ncols))\n",
    "    figsize = (4*ncols,4*nrows)\n",
    "    ax = np.zeros((nrows, ncols))\n",
    "\n",
    "    # Choose which colors to use for each line\n",
    "    beg_color = Color(\"red\")\n",
    "    end_color = Color(\"blue\")\n",
    "    increasing_colors = list(beg_color.range_to(end_color,len(features)))\n",
    "    \n",
    "    # Create instances for the figure and all of the subplots within the figure\n",
    "    fig, ax = plt.subplots(nrows=nrows, ncols=ncols, sharex='all', sharey='all', figsize=figsize, squeeze = False)\n",
    "    for i in range(nrows):\n",
    "        for j in range(ncols):\n",
    "\n",
    "            # Pick each stock based on location within the nrows x ncols figure\n",
    "            stockIndex = i*ncols + j\n",
    "            if stockIndex < len(stockNames):\n",
    "                ax[i,j].set_title(stockNames[stockIndex])\n",
    "                for k, line in enumerate(features):\n",
    "\n",
    "                    # Initialize the y variable with max_time_index\n",
    "                    y = pd.Series(None, index=max_time_index)\n",
    "                    y.update(base_data.loc[stockNames[stockIndex], line]) \n",
    "                    # Plot the line\n",
    "                    lined = ax[i,j].plot(x, y, color=increasing_colors[k].rgb)\n",
    "    fig.legend(lined, ('Line 1'), 'upper left')\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's the data tables look like now.\n",
    "print_graphs(base_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(base_data):\n",
    "    \n",
    "    # Note: This standardizes across all stock data, not for each individual stock.\n",
    "    \n",
    "    # Cycle through each column in the database and standardize each column\n",
    "    for col in base_data.columns:\n",
    "        \n",
    "        # Don't standardize the \"is_sellable\" column\n",
    "        if col != \"is_sellable\":\n",
    "            mean = base_data.loc[:, col].mean()\n",
    "            stddev = base_data.loc[:, col].std()\n",
    "            base_data.loc[:, col] = (base_data.loc[:, col] - mean) / stddev\n",
    "    \n",
    "    return base_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll need to standardize the data to a Gaussian normal (mean = 0, std dev = 1) to allow the model to train faster\n",
    "base_data = standardize(base_data)\n",
    "base_data.to_csv(\"standardized.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graphs below show the distribution of each column after standardizing them. Notice that oh_pct and ol_pct don't have the normal distribution shape - that is because by the nature of their data. High prices are always higher than open prices, thus oh_pct is always positive, and ol_pct is always negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_data.hist(bins=100, sharex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = {}\n",
    "trading_days = get_trading_days(base_data)\n",
    "for stock in stockNames:\n",
    "    m[stock] = trading_days[stock] - (p + window) + 1\n",
    "\n",
    "print(np.arange(m[stock]))\n",
    "print(trading_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_for_LSTM(base_data):\n",
    "    \n",
    "    # Re-shaping the data set. Each row should represent `window` days \n",
    "    # of trading data and the sell flag.\n",
    "\n",
    "    # We're going to be converting our data from pandas to numpy, \n",
    "    # which supports multi-dimensional data much better.\n",
    "    \n",
    "    # Track how long it takes to do this. Performance affected by for loop.\n",
    "    startTime = time.time()\n",
    "    \n",
    "    # Identify the stocks within the base_data\n",
    "    stockNames = base_data.index.levels[0].values\n",
    "    \n",
    "    # Identify the number of trading days uploaded for each stock\n",
    "    trading_days = get_trading_days(base_data)\n",
    "        \n",
    "    # Identify the number of data samples we can get for each stock\n",
    "    m = {}\n",
    "    trading_days = get_trading_days(base_data)\n",
    "    for stock in stockNames:\n",
    "        m[stock] = trading_days[stock] - (p + window) + 1\n",
    "    \n",
    "    f = len(features)\n",
    "    # Intialize the numpy dataset with the right dimensions: (m, p, features)\n",
    "    # We'll add 1 extra slot onto the 2nd dimension so we can include \n",
    "    # the is_sellable column and shuffle the dataset together. \n",
    "    # After shuffling, we'll separate out is_sellable as our y variables.\n",
    "    datanp = np.zeros((sum(m.values()), p + 1, f))   \n",
    "\n",
    "    # Use i_base so we can count across rows for each stock as well as total rows in datanp\n",
    "    i_base = 0\n",
    "    for stock in stockNames:\n",
    "        for i in np.arange(m[stock]): \n",
    "            datanp[i+i_base, :p, :] = base_data.loc[stock, features].iloc[i:i+p].values\n",
    "            datanp[i+i_base, -1, 0] = base_data.loc[stock, \"is_sellable\"].iloc[i+p]\n",
    "        i_base += m[stock]\n",
    "        \n",
    "    return datanp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll convert and re-shape the base_data into a suitable format for our LSTM model.\n",
    "# The data will be re-shaped to have shape (m, p+1, # of features)\n",
    "# m is the number of data samples we'll have. Because we're looking at a \n",
    "# history of days size `p`, with a look forward of `window`, our final m count will be\n",
    "# m = trading_days[stock] - (p + w) + 1\n",
    "\n",
    "datanp = reshape_for_LSTM(base_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a number index to 3 dimension, index 1 so that we can review what the data looks like.\n",
    "# This column won't be copied over to X and Y anyways.\n",
    "datanp[:,-1,1] = np.arange(datanp.shape[0])\n",
    "for col in np.arange(datanp.shape[2]):\n",
    "    np.savetxt(\"index\"+str(col)+\".csv\", datanp[:,:,col], fmt='%.3f', delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to shuffle the data in order to train the model well.\n",
    "# Otherwise we introduce unnecessary bias.\n",
    "#np.random.shuffle(datanp)\n",
    "#for col in np.arange(datanp.shape[2]):\n",
    "#    np.savetxt(\"shuffle\"+str(col)+\".csv\", datanp[:,:,col], fmt='%.3f', delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we'll separate out the X data from the Y data\n",
    "X = datanp[:, :p, :]\n",
    "Y = datanp[:, -1, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need to split our data set into a training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split our data set into train/test sets based on split %.\n",
    "pct_train = .95\n",
    "pct_test = 1 - pct_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index of separation points within the index\n",
    "train_test_idx_num = int(pct_train * X.shape[0])\n",
    "\n",
    "# Split the data as per proportions above\n",
    "X_train = X[:train_test_idx_num, :, :]\n",
    "Y_train = Y[:train_test_idx_num]\n",
    "\n",
    "X_test = X[train_test_idx_num:, :, :]\n",
    "Y_test = Y[train_test_idx_num:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is what the dimensions look like\n",
    "display(\"Shape of X_train: {}\".format(X_train.shape))\n",
    "display(\"Shape of Y_train: {}\".format(Y_train.shape))\n",
    "display(\"Shape of X_test: {}\".format(X_test.shape))\n",
    "display(\"Shape of Y_test: {}\".format(Y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll begin building our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for setting up the model architecture\n",
    "\n",
    "def model_setup_2_layers():\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(input_shape = (X_train.shape[1], X_train.shape[2]), units = 50, return_sequences = True))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(LSTM(100, return_sequences = False))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 2 layer model, and see how it performs.\n",
    "model = model_setup_2_layers()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition for training the model\n",
    "def train_model(model, epochs, batch_size):\n",
    "    start = time.time()\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    print ('Model compilation time : ', time.time() - start)\n",
    "    history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size)\n",
    "    print ('Model fit time : ', time.time() - start)\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "\n",
    "# The number of times we'll run our entire data set through the model to train it\n",
    "epochs = 50\n",
    "\n",
    "# Batch size is the number of X samples that will run through for each iteration. \n",
    "# Usually we just pick a number between 1 and m to speed up training.\n",
    "batch_size = 32\n",
    "\n",
    "# Train the model (using my custom function)\n",
    "model, history = train_model(model, epochs = epochs, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot the loss and accuracy functions\n",
    "fn.plot_loss(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model with our test set\n",
    "score = model.evaluate(X_test, Y_test, batch_size=128)\n",
    "print(\"Baseline average of Y:    {:3.1f}%\".format(np.average(Y)*100))\n",
    "print(\"Y_test prediction:        {:3.1f}%\".format(score[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like for the mixture of stocks, this model didn't improve predictions at all!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further investigations:\n",
    "- Review for any other journals on RNN models on stock data\n",
    "- Check other ML models - what did they do? What did they include? What did they not include? And why?\n",
    "- Do we need to add some drop out blocks? Drop out blocks help with regularization. But given that there is so much noise in stock data, is this required?\n",
    "- Do I need to further update the data so that it's not just raw prices, but daily changes, volumes, distance, etc.?\n",
    "\n",
    "This research was inspired by the following papers:\n",
    "- https://pdfs.semanticscholar.org/6ea2/b0f7f9de845790d9add00a0a103f9d3242b5.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
